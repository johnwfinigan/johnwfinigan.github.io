<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>johnwfinigan.github.io</title>
  <link rel="self" type="application/atom+xml" href="https://johnwfinigan.github.io/atom.xml"/>
  <updated>2023-05-08T00:00:00Z</updated>
  <author>
    <name>John Finigan</name>
  </author>
  <id>tag:johnwfinigan.github.io,2015-09-14:blog</id>
  <rights> Copyright 2015-2023 John Finigan </rights>
<entry>
<id>tag:johnwfinigan.github.io,2023-05-08:Using-IAP-SSH-to-Run-Ansible-in-Google-Cloud-Build</id>
<title>Using IAP SSH to run Ansible in Google Cloud Build Triggers</title>
<updated>2023-05-08T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Using IAP SSH to run Ansible in Google Cloud Build Triggers
       Google pitches Cloud Build as the serverless CI/CD platform for
       Google Cloud (GCP). Its easy to use for infrastructure tasks like
       deploying cloud infrastructure using Terraform. This is great for
       ensuring that your Terraform build environment is repeatable and not
       tied to implicit state on a build machine.

       When a Cloud Build run is triggered, GCP runs a container of your
       choosing on a network that is, by default, not part of your customer-
       controlled VPC network. By default, traffic egressing from your Cloud
       Build will be seen by your instances as originating from a public IP of
       Googles choosing. This can be a problem if you want to use Cloud Build
       to run Ansible to configure your instances, since Ansible will
       typically log in to your instances using SSH. You could allow SSH
       traffic in from any public IP, but this is bad security practice if
       your only reason for doing this is admin access.

       Another option is to do the config necessary to give Cloud Build access
       to your customer-controlled VPC network, by using Private Pools, which
       give a limited amount of control over the machine size and networking
       of the virtual hardware your builds run on. However, this requires some
       configuration work &lt;https://cloud.google.com/build/docs/private-
       pools/set-up-private-pool-to-use-in-vpc-network&gt;, including the use of
       Private Services Access to connect to your VPC network by VPC peering.
       Personally, I dont find this appealing. It injects a fair amount of
       accidental complexity for this simple use case.

       But, there is another way. Google Cloud offers something called
       Identity Aware Proxy (IAP), which allows you to tunnel TCP traffic from
       anywhere to inside your VPC network, without your instances having
       public IPs. The gcloud compute ssh command has built in IAP support. I
       use this extensively for general admin access. It generally just
       works.

       All that remains is to plumb IAP SSH into Ansible, and do it in such a
       way that it can run in Cloud Build.

       The strategy here is to get Ansible to use gcloud compute ssh as its
       SSH provider, and to pass inventory between them in such a way that
       gcloud can recognize the instances, which essentially means using the
       instance name as defined in GCE.

       Below is the Dockerfile for a runtime container that is compatible with
       Cloud Build:

       FROM gcr.io/google.com/cloudsdktool/google-cloud-cli:slim

       RUN $(gcloud info --format=&quot;value(basic.python_location)&quot;) -m pip install numpy
       RUN python3 -m pip install ansible

       ENV ANSIBLE_SSH_EXECUTABLE=/bin/g-ssh.sh
       ENV ANSIBLE_CONFIG=/etc/g-ansible.cfg

       COPY g-ssh.sh $ANSIBLE_SSH_EXECUTABLE
       COPY g-ansible.cfg $ANSIBLE_CONFIG

       This is a Debian-based Google Cloud SDK container. In my testing, an
       Alpine container had significantly worse performance for Ansible over
       IAP SSH. Numpy is installed to speed up IAP forwarding performance. In
       my testing, this improved performance significantly.

       Inside this container is g-ssh.sh, where the IAP SSH magic happens.

       #!/bin/bash

       umask 0077

       # generate an ephemeral ssh key. exists only for this build step
       if [ ! -f &quot;$HOME/.ssh/google_compute_engine&quot; ] ; then
	 mkdir -p &quot;$HOME/.ssh&quot;
	 ssh-keygen -t rsa -b 3072 -N &quot;&quot; -C &quot;cloudbuild-$(date +%Y-%m-%d-%H-%M-%S)&quot; -f $HOME/.ssh/google_compute_engine
       fi

       #
       # adapted from https://unix.stackexchange.com/questions/545034/with-ansible-is-it-possible-to-connect-connect-to-hosts-that-are-behind-cloud-i
       #

       # get the two rightmost args to the script
       host=&quot;${@: -2: 1}&quot;
       cmd=&quot;${@: -1: 1}&quot;

       # controlmasters is a performance optimization, added because gcloud ssh initiation is relatively slow
       mkdir -p /workspace/.controlmasters/
       # stagger parallel invocations of gcloud to prevent races - not clear how useful
       flock /workspace/.lock1 sleep 1

       # note that the --ssh-key-expire-after argument to gcloud is set, meaning that the ephemeral key
       # will become invalid in OSLogin after one hour. Otherwise will end up with dozens of junk keys in OSLogin
       gcloud_args=&quot; --ssh-key-expire-after=1h --tunnel-through-iap --quiet --no-user-output-enabled -- -C -o ControlPath=/workspace/.controlmasters/%C -o ControlMaster=auto -o ControlPersist=300 -o PreferredAuthentications=publickey -o KbdInteractiveAuthentication=no -o PasswordAuthentication=no -o ConnectTimeout=20 &quot;

       # project and zone must be already set using gcloud config
       exec gcloud compute ssh &quot;$host&quot; $gcloud_args &quot;$cmd&quot;

       Note that an ephemeral SSH key is generated but has an expiration set.
       This key is lost when the build ends, and GCP will remove it from the
       authorized keys list when it expires after one hour. This is done using
       the --ssh-key-expire-after argument to gcloud. Thus there are no long
       lived SSH credentials involved here. I believe this SSH key management
       feature depends on the use of Google OSLogin on the instances.

       This leads the broader subject of roles and identity. The identity that
       is logging in to the instance and running Ansible commands will be the
       service account that your Cloud Build build runs as. This setup depends
       on that account being a valid OSLogin + IAP instance accessor account.
       That requires:

	There must be a firewall rule on your VPC networks that allows IAP
	 SSH &lt;https://cloud.google.com/iap/docs/using-tcp-forwarding#create-
	 firewall-rule&gt; to reach instance private IPs from Googles designated
	 IAP origination range. The instances do not require public IPs.

	The instances must have OSLogin Enabled
	 &lt;https://cloud.google.com/compute/docs/oslogin/set-up-oslogin&gt;

	The Cloud Build service account you use must have the following
	 roles:

	  roles/compute.osAdminLogin on the instance or project
	  roles/iam.serviceAccountUser on the instances service account
	  roles/iap.tunnelResourceAccessor on the project or tunnel

       I have not tested with the default Cloud Build service account, but
       only with cloud build running as a custom service account. The Service
       Account User role may seem surprising, but OSLogin requires this for
       any account to be able to log in. I believe it is because any account
       that logs in to an instance can make requests using its service
       account, so this formalizes the relationship.

       Finally, in your container, you need a small Ansible config file,
       g-ansible.cfg. Ive been using this one for Ansible+IAP for years and
       dont remember all the details, but some tweaks were required to get
       file transfer (eg. Ansible copy:) to work reliably. You may have to
       change the default interpreter_python based on the Linux version you
       run in your instances. I had to limit forks for stability, but have not
       extensively debugged to see what could be done to speed this up again.

       [ssh_connection]
       pipelining = True
       ssh_args =
       transfer_method = piped

       [defaults]
       forks = 1
       interpreter_python = /usr/bin/python3

       Heres a sample cloudbuild.yaml that uses this container to run Ansible
       through IAP in Cloud Build:

       steps:

       - id: &#39;test ansible&#39;
	 name: &#39;us-central1-docker.pkg.dev/my_project/cloudbuild-containers/cloudbuild-ansible-iap&#39;
	 entrypoint: &#39;/bin/bash&#39;
	 args:
	 - &#39;-c&#39;
	 - |
	   gcloud config set project my_example_project;
	   gcloud config set compute/zone us-central1-a ;
	   ansible-playbook -i ./test.ini test.yml

       Note that you must set the project and zone in your build step. That is
       how g-ssh.sh knows how to find your instances by name.

       This has worked well for me. Speed has been acceptable, but a more
       optimized build might create the IAP tunnels directly instead of using
       the gcloud compute ssh helper. This would allow using unwrapped openssh
       to access the instances, but would require some more complex state
       tracking that I have not yet spent time on.

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2023-02-05:Creating-a-Minimal-deb-Repo-From-Scratch</id>
<title>Creating a Minimal apt Repo From Scratch</title>
<updated>2023-02-05T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Creating a Minimal apt Repo From Scratch
       Lets say you want to create an apt repo for debs you have built
       yourself. I spent an hour or two cobbling together these minimalist
       instructions from various internet sources. The sources I found seemed
       to be either outdated enough that they were no longer correct for
       recent tooling and distros, or more complex than what I was trying to
       accomplish warranted. This is tested against Ubuntu 22.04 clients and
       servers. Notably, this method has no directory structure inside the
       repo at all. All files are under the top level directory of the repo.

   Create a gpg key for signing
       Use the gpg shipped with Ubuntu 22.04 or whatever client distro you
       plan to support. Set the Real Name field of your key to whatever you
       like. Here, its called Mirror Signing Key. Once generated, export your
       key using the fingerprint shown when it was generated.

       gpg --armor --export 73341B91FEC7DCBC8316EE01E45BEE2E63B81095 &gt; MirrorSigningKey.pub

       This exported key will be copied to your apt clients for apt metadata
       verification.

       Example gpg session at the bottom of this post. In short, its OK to
       choose RSA and RSA, and set RSA key size to 4096.

   Populate your repo
       Put your debs in a directory that you intend to serve to your clients.
       Here, its called $repo_dir

   Install repo build tools
       Ensure you have the packages dpkg-deb and apt-utils installed, for the
       next step.

   Create your apt repo
       #!/bin/bash

       set -eu

       repo_dir=/path/to/your/repo/directory
       cd &quot;$repo_dir&quot;

       rm -vf Release Release.gpg InRelease
       dpkg-scanpackages --arch amd64 . &gt; Packages
       apt-ftparchive release . &gt; Release

       echo Enter Passphrase
       read pass
       gpg  --pinentry-mode loopback --digest-algo SHA512 --batch --yes --no-tty --passphrase $pass --default-key &#39;Mirror Signing Key&#39; -abs &lt; Release &gt; Release.gpg
       gpg  --pinentry-mode loopback --digest-algo SHA512 --batch --yes --no-tty --passphrase $pass --default-key &#39;Mirror Signing Key&#39; -abs --clearsign &lt; Release &gt; InRelease
       unset pass

       The awkward method of getting the passphrase to gpg is not appropriate
       for use on untrusted systems, since the passphrase can be read out of
       /proc while gpg is executing. Proper gpg pin entry methods seem
       especially fragile on headless systems, and after an hour of
       frustration trying to debug pinentry, I resorted to it. Someone with
       better gpg skills could do better. On the other hand, its easy to see
       how this could be modified to work with a secret manager in a CI system
       - just replace the read call with a call to your secret manager.

   Set up your clients
       On your clients, create the repo definition file and the public signing
       key file in /etc/apt

       install -o root -g root -m0644 &lt;(echo &#39;deb [signed-by=/etc/apt/trusted.gpg.d/MirrorSigningKey.pub] https://example.org/your/repo ./&#39;) /etc/apt/sources.list.d/your_repo.list

       install -o root -g root -m0644 MirrorSigningKey.pub /etc/apt/trusted.gpg.d/

       Note that Im using install here as an all-in-one way to copy data
       while ensuring that permissions are reasonable. Your preferred way to
       do that should be fine, regardless.

   Done
       When you add, update, or remove packages from your repo, simply rerun
       the repo generation script above.

   Appendix: GPG sample session
       john@s:~$ gpg --full-generate-key
       gpg (GnuPG) 2.2.27; Copyright (C) 2021 Free Software Foundation, Inc.
       This is free software: you are free to change and redistribute it.
       There is NO WARRANTY, to the extent permitted by law.

       gpg: directory &#39;/home/john/.gnupg&#39; created
       gpg: keybox &#39;/home/john/.gnupg/pubring.kbx&#39; created
       Please select what kind of key you want:
	  (1) RSA and RSA (default)
	  (2) DSA and Elgamal
	  (3) DSA (sign only)
	  (4) RSA (sign only)
	 (14) Existing key from card
       Your selection? 1
       RSA keys may be between 1024 and 4096 bits long.
       What keysize do you want? (3072) 4096
       Requested keysize is 4096 bits
       Please specify how long the key should be valid.
		0 = key does not expire
	     &lt;n&gt;  = key expires in n days
	     &lt;n&gt;w = key expires in n weeks
	     &lt;n&gt;m = key expires in n months
	     &lt;n&gt;y = key expires in n years
       Key is valid for? (0)
       Key does not expire at all
       Is this correct? (y/N) y

       GnuPG needs to construct a user ID to identify your key.

       Real name: Mirror Signing Key
       Email address: mirrorsignkey@example.org
       Comment:
       You selected this USER-ID:
	   &quot;Mirror Signing Key &lt;mirrorsignkey@example.org&gt;&quot;

       Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O
       We need to generate a lot of random bytes. It is a good idea to perform
       some other action (type on the keyboard, move the mouse, utilize the
       disks) during the prime generation; this gives the random number
       generator a better chance to gain enough entropy.


       gpg: /home/john/.gnupg/trustdb.gpg: trustdb created
       gpg: key E45BEE2E63B81095 marked as ultimately trusted
       gpg: directory &#39;/home/john/.gnupg/openpgp-revocs.d&#39; created
       gpg: revocation certificate stored as &#39;/home/john/.gnupg/openpgp-revocs.d/73341B91FEC7DCBC8316EE01E45BEE2E63B81095.rev&#39;
       public and secret key created and signed.

       pub   rsa4096 2023-02-06 [SC]
	     73341B91FEC7DCBC8316EE01E45BEE2E63B81095
       uid			Mirror Signing Key &lt;mirrorsignkey@example.org&gt;
       sub   rsa4096 2023-02-06 [E]

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2022-09-13:Firewalling-Linux-NFS-Servers-and-OpenBSD-NFS-Clients</id>
<title>Firewalling Linux NFS Servers and OpenBSD NFS Clients</title>
<updated>2022-09-13T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Firewalling Linux NFS Servers and OpenBSD NFS Clients
       OpenBSD supports only NFS v3. It does not support NFS v4. While NFS v4
       requires only port 2049/tcp to be open between clients and servers, NFS
       v3 typically requires three or four ports open between a client and a
       server, and most of these are dynamically chosen and have no fixed port
       number. NFS v3 predates the widespread use of firewalls and was not
       designed to be easily firewalled.

       In order to firewall NFS v3, the server must be configured to run all
       needed NFS v3 services on fixed ports. On an OpenBSD client, no special
       configuration is required on the mount. However, I recommend
       configuring the mount to run over tcp, both to make firewalling easier
       and for general robustness. An OpenBSD fstab entry like the following
       is enough:

       172.16.1.6:/srv/data /data nfs rw,tcp 0 0

       For the Linux server configuration, I will use Ansible and Firewalld,
       but theres nothing special about these and it should be easy to adapt
       this solution to any firewall or config management method. Ubuntu 22.04
       has most NFS server config in /etc/nfs.conf, while older distros such
       as Ubuntu 20.04 require editing multiple config files. I have
       configurations for both, below:

   Linux NFS Server Firewalld config
       NFS client is at 172.16.1.77

       ---
       - name: NFS server firewall config
	 become: yes
	 hosts: all
	 tasks:


	 - name: firewalld rich rules
	   ansible.posix.firewalld:
	     rich_rule: &quot;{{ item }}&quot;
	     zone: public
	     permanent: yes
	     immediate: yes
	     state: enabled
	   loop:
	     - rule family=&quot;ipv4&quot; source address=&quot;172.16.1.77/32&quot; port port=&quot;111&quot; protocol=&quot;tcp&quot; accept
	     - rule family=&quot;ipv4&quot; source address=&quot;172.16.1.77/32&quot; port port=&quot;111&quot; protocol=&quot;udp&quot; accept
	     - rule family=&quot;ipv4&quot; source address=&quot;172.16.1.77/32&quot; port port=&quot;50001-50003&quot; protocol=&quot;tcp&quot; accept

   Ubuntu 22.04 NFS Server Config
       /etc/nfs.conf is in ini file format. Ansible ini_file makes it easy to
       edit ini programatically, but if youre editing by hand, you can easily
       pull out the section name, option (key) names, and values below:

	 - name: nfs v3 port locking for ubuntu 22.04 server
	   ini_file:
	     path: /etc/nfs.conf
	     backup: yes
	     section: &quot;{{ item.s }}&quot;
	     option: &quot;{{ item.o }}&quot;
	     value:  &quot;{{ item.v }}&quot;
	   loop:
	     - { s: &quot;statd&quot;, o: &quot;port&quot;, v: &quot;50001&quot; }
	     - { s: &quot;statd&quot;, o: &quot;outgoing-port&quot;, v: &quot;50000&quot; }
	     - { s: &quot;mountd&quot;, o: &quot;port&quot;, v: &quot;50003&quot; }
	     - { s: &quot;lockd&quot;, o: &quot;port&quot;, v: &quot;50002&quot; }
	     - { s: &quot;lockd&quot;, o: &quot;udp-port&quot;, v: &quot;50004&quot; }

       All of the various NFS server-related services must be restarted to
       have this take effect. systemctl restart nfs-server was not sufficient
       for me. After much fooling around, I gave up and rebooted. I believe
       the problem may be that some of these services can listen on multiple
       ports, and a restart causes them to start listening on their new ports,
       but not abandon their old ports. Which port a new mount request gets is
       then unpredictable.

       Note that we are pinning the lockd UDP port to 50004 for thoroughness,
       but the tcp-only client will not use it.

   Ubuntu 20.04 NFS Server Config
       This configuration, or a close variation on it, should work for any
       older Ubuntu or RHEL version from the last decade or so. Will not take
       effect until all NFS server-related services are restarted.

	  - name: port locked nfs config 1
	    lineinfile:
	      line: &#39;options lockd nlm_udpport=50004 nlm_tcpport=50002&#39;
	      path: /etc/modprobe.d/nfs-lockd.conf
	      mode: 0644
	      owner: root
	      group: root
	      backup: yes
	      create: yes

	  - name: port locked nfs config 2
	    lineinfile:
	      line: &#39;STATDOPTS=&quot;--port 50001 --outgoing-port 50000&quot;&#39;
	      path: /etc/default/nfs-common
	      backup: yes
	      regexp: &#39;^STATDOPTS=&#39;


	  - name: port locked nfs config 3
	    lineinfile:
	      line: &#39;RPCMOUNTDOPTS=&quot;--manage-gids --port 50003&quot;&#39;
	      path: /etc/default/nfs-kernel-server
	      backup: yes
	      regexp: &#39;^RPCMOUNTDOPTS=&#39;

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2015-09-17:installing-nagios-from-epel-on-centos-7-with-gmail-notifications</id>
<title>Minimum viable install: Nagios from epel on Centos 7, with Gmail notifications</title>
<updated>2015-09-17T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Minimum viable install: Nagios from epel on Centos 7, with Gmail
       notifications
       (Historical post - this is outdated, but parts are likely still useful.
       It begs for scripting and templating)

       This is how I got basic Nagios running on Centos 7, using the Nagios
       3.5.1 packages in EPEL &lt;https://fedoraproject.org/wiki/EPEL&gt;. This is
       basically a reinterpretation of the guide at the Nagios Fedora
       Quickstart
       &lt;https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/3/en/quickstart-
       fedora.html&gt;, except we are installing from packages and not source,
       and a lot of the paths and commands in Centos 7 are different.

       First, to get a basic install running:

   As root:
	rpm -i https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
	yum install  mod_ssl git nagios*

	chkconfig nagios on
	systemctl enable httpd
	systemctl enable firewalld
	service firewalld start
	firewall-cmd --zone=public --permanent --add-service=http
	firewall-cmd --zone=public --permanent --add-service=https

       You cant add the permanent firewalld rules unless firewalld is already
       running, so we started it.

   Next, edit /etc/httpd/conf.d/nagios.conf
	Remove 2 instances of:

	AuthType Basic
	AuthUserFile /etc/nagios/passwd

	Replace with:

	AuthType Digest
	AuthUserFile /etc/nagios/digest-passwd


	Optionally, uncomment 2 instances of SSLRequireSSL

       This accomplishes two things. Firstly, it changes the HTTP
       authentication method so passwords are never sent in cleartext, but
       rather in hashed form. Secondly, and optionally, it denies access to
       Nagios over http, and enforces the use of https. Edit
       /etc/httpd/conf.d/ssl.conf to point it to a signed ssl certificate if
       desired.

   As root:
       In order to use digest passwords instead of plaintext passwords, we
       have to create the digest password file:

	htdigest -c /etc/nagios/digest-passwd &quot;Nagios Access&quot; nagiosadmin
	# will prompt for password

	chown root:apache /etc/nagios/digest-passwd
	chmod 640 /etc/nagios/digest-passwd

       Next, reboot! This is partially out of laziness so we dont have to
       start the services manually, and partially due to a virtuous desire to
       see if our services autostart as requested.

       reboot

       At this point you should be able to log in to Nagios on both http and
       https.

   Next, edit /etc/nagios/objects/contacts.cfg
       Find the line containing CHANGE THIS TO YOUR EMAIL ADDRESS , and do what it asks.

       Append a sample notification contact and contact group to the end of the file:
       Don&#39;t forget to change the email address below.

       define contactgroup {
	       contactgroup_name       testgroup
	       alias		       Test Contact Group
	       members		       testsysadmin
       }

       define contact {
	       contact_name		       testsysadmin
	       alias			       Test-Sysadmin
	       service_notification_period     24x7
	       host_notification_period	       24x7
	       service_notification_options    c,r
	       host_notification_options       d,r
	       service_notification_commands   notify-service-by-email
	       host_notification_commands      notify-host-by-email
	       email			       sysadmin@example.com   &lt;-- CHANGE THIS!
	}

   Define some things to monitor
       By default, what you put in /etc/nagios/conf.d/ gets monitored.

       cd /etc/nagios/conf.d

       This directory starts empty and you can add config files organized
       however you like. Heres one barebones layout, as an example. Dont
       copy and paste, since the hostnames are up to you.

       Contents of file /etc/nagios/conf.d/hosts.cfg

       define hostgroup{
	       hostgroup_name  all-servers ; The name of the hostgroup
	       alias	       All Servers ; Long name of the group
       }

       define host{
	       use	       linux-server	       ; Inherit default values from a template
	       host_name       myserver.example.com    ; The name we&#39;re giving to this host
	       alias	       myserver		       ; A longer name associated with the host
	       address	       10.0.0.10
	       hostgroups      all-servers	       ; Host groups this host is associated with
       }

       Contents of file /etc/nagios/conf.d/ssh.cfg

       define service{
	   use		   generic-service
	   host_name	   myserver.example.com
	   service_description SSH
	   check_command	   check_ssh
       }

       Contents of file /etc/nagios/conf.d/http.cfg

       define service{
	   use		   generic-service
	   host_name	   myserver.example.com
	   service_description HTTP
	   check_command	   check_http
       }

       define service{
	   use		   generic-service
	   host_name	   myserver.example.com
	   service_description HTTP
	   check_command	   check_http!-S -H mysite.example.com	; checks https on a different virtual host
       }

       Important fact: these check commands live in /usr/lib64/nagios/plugins,
       and can be run manually, including to get help.

       /usr/lib64/nagios/plugins/check_http -h

   Test your config
       If it works, restart Nagios to pick up config file changes. You will do
       this a lot.

       /usr/sbin/nagios -v /etc/nagios/nagios.cfg
       service nagios restart

   Gmail Notifications
       Finally, lets get some notifications going. Nagios is ready to notify
       as soon as we have a working mta. I use gmail, and dont want to run a
       real MTA like sendmail. Taking inspiration from here
       &lt;http://sharadchhetri.com/2013/07/16/how-to-use-email-id-of-gmail-for-
       sending-nagios-email-alerts/&gt;, we will use the ssmtp send-only
       sendmail emulator, but moving around binaries like in the linked post
       makes me nervous since the moves will be clobbered by future Centos
       updates, so well use the alternatives system to subsititute ssmtp for
       sendmail instead.

       Be very careful if you are already running a MTA like sendmail on your
       nagios machine and want it to keep working.

       yum install ssmtp
       alternatives --install /sbin/sendmail sendmail /sbin/ssmtp 100

       Then append the following to /etc/ssmtp/ssmtp.conf

       AuthUser=my-nagios-sender@gmail.com   &lt;-- CHANGE THIS!
       AuthPass=password123		     &lt;-- CHANGE THIS!
       FromLineOverride=YES
       mailhub=smtp.gmail.com:587
       UseSTARTTLS=YES

   Done!
       This is a start anyway. You can spend as much time as you like refining
       your config; if anything, the difficulty in getting started is that
       there are so many options.

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2015-09-14:installing-suse-linux-sles-11-on-qemus-s390x-emulated-mainframe</id>
<title>Installing SUSE Linux SLES 11 on qemu's s390x emulated mainframe with virtio</title>
<updated>2015-09-14T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Installing SUSE Linux SLES 11 on qems s390x emulated mainframe with
       virtio
       (Historical post - this is outdated, but parts are likely still useful)

       Installing Linux on an emulated mainframe can be confusing unless you
       are already a mainframer and you have good install docs for your
       specific distro. The traditional choice of emulators is Hercules
       &lt;http://www.hercules-390.eu&gt;, but Hercules is written by mainframers,
       for mainframers, and there is a lot to learn if you want to build your
       config from scratch.

       Traditional mainframe Linux does not install or boot like amd64 Linux.
       Hercules is designed to run traditional mainframe operating systems
       like z/OS, and so presents hardware to the guest that looks like what
       z/OS expects to see. There is a lot to understand if you just want to
       try your program on s390x Linux.

       However, QEMs &lt;http://wiki.qemu.org/Main_Page&gt; s390x abilities have
       improved recently due to IBMs adoption of KVM virtualization on z
       Systems as an alternative to z/VM. Most importantly, recent QEMU has
       new virtio paravirtual IO devices for s390x Linux, meaning that you do
       not need to configure emulated mainframe channel controllers and DASD.

       All of this would not help if mainframe QEMU was only useful for KVM.
       But its not: the qemu-system-s390x emulator works just fine.

       I used QEMU 2.4, built from source. Everything else came with my Ubuntu
       15.04 install. I doubt I would have figured any of this out without
       looking at this SHARE presentation by Mark Post
       &lt;https://share.confex.com/share/125/webprogram/Handout/Session17489/know.your.competition.pdf&gt;

       In order for this to work, you will need a mainframe Linux distro that
       supports virtio mainframe disk and network. Since this is a recent
       addition, most mainframe Linux distros do not have a kernel that
       supports it. SUSE Linux Enterprise Server 11 SP4 is new enough. You can
       get a trial here &lt;https://www.suse.com/products/server/download/&gt;

       Too new may also not work: Apparently RHEL 7 and SLES 12 wot
       &lt;https://lists.gnu.org/archive/html/qemu-devel/2015-08/msg03884.html&gt;.
       The whole linked discussion is worth reading.

       Finally, its convenient to do the install over HTTP since real
       mainframes rarely install from CD, so CD is not the path of least
       resistance.

       To prep, get yourself a copy of the SUSE ISO and create a virtual disk
       file for your root drive:

       $ mkdir $HOME/qemu.test ; cd $HOME/qemu.test
       $ qemu-img create -f qcow2 SLES-11-SP4-s390x.qcow2 20G
       $ ls
       SLES-11-SP4-DVD-s390x-GM-DVD1.iso  SLES-11-SP4-s390x.qcow2

       Now, you will need an initrd and kernel to start QEMU with. The initrd
       is on the ISO, but we have to work a little for the kernel:

       $ mkdir mnt
       $ sudo mount ./SLES-11-SP4-DVD-s390x-GM-DVD1.iso ./mnt
       $ cp mnt/boot/s390x/initrd .
       $ mkdir junk ; cd junk
       $ rpm2cpio ../mnt/suse/s390x/kernel-default-base-3.0.101-63.1.s390x.rpm | cpio -ivd
       $ zcat boot/vmlinux-3.0.101-63-default.gz &gt; ../kernel
       $ cd .. ; rm -rf junk

       Finally, serve the install disks contents locally using HTTP:

       $ cd ./mnt ; python -m SimpleHTTPServer

       Now, in a new terminal, the moment weve all been waiting for::

       $ cd $HOME/qemu.test
       $ qemu-system-s390x -M s390-ccw-virtio -m 1024 -smp 1 -nographic \
	 -drive file=SLES-11-SP4-s390x.qcow2,format=qcow2,if=none,id=drive-virtio-disk0 \
	 -device virtio-blk-ccw,drive=drive-virtio-disk0,id=virtio-disk0 \
	 -netdev user,id=mynet0,hostfwd=tcp::10022-:22 \
	 -device virtio-net-ccw,netdev=mynet0,id=net0,mac=08:00:2F:00:11:22,devno=fe.0.0001 \
	 -kernel ./kernel -initrd ./initrd

       A couple of networking notes: We are using QEMUs user networking
       option here, which uses QEMUs internal NAT gateway and DHCP server,
       but is slow. It is zero setup though, which is why were using it. The
       hostfwd=tcp::10022-:22 argument forwards port 22 (SSH) on the guest to
       port 10022 on the host. The MAC address is in PR1MEs space, so make
       sure it does not conflict with any PRIMOS
       &lt;https://en.wikipedia.org/wiki/PRIMOS&gt; systems you may be running.

       Heres a log of me running through the beginning of the setup once the
       guest is booted. 10.0.2.2 is QEMUs emulated router and maps to the
       host.

       &gt;&gt;&gt; Linuxrc v3.3.108 (Kernel 3.0.101-63-default) &lt;&lt;&lt;

       Main Menu

       0) &lt;-- Back &lt;--
       1) Start Installation
       2) Settings
       3) Expert
       4) Exit or Reboot

       &gt; 1

       Start Installation

       0) &lt;-- Back &lt;--
       1) Start Installation or Update
       2) Boot Installed System
       3) Start Rescue System

       &gt; 1

       Choose the source medium.

       0) &lt;-- Back &lt;--
       1) DVD / CD-ROM
       2) Network

       &gt; 2

       Choose the network protocol.

       0) &lt;-- Back &lt;--
       1) FTP
       2) HTTP
       3) HTTPS
       4) NFS
       5) SMB / CIFS (Windows Share)
       6) TFTP

       &gt; 2
       Detecting and loading network drivers

       Automatic configuration via DHCP?

       0) &lt;-- Back &lt;--
       1) Yes
       2) No

       &gt; 1
       Sending DHCP request...
       8021q: adding VLAN 0 to HW filter on device eth0

       Enter the IP address of the HTTP server. (Enter &#39;+++&#39; to abort).
       &gt; 10.0.2.2:8000

       Enter the directory on the server. (Enter &#39;+++&#39; to abort).
       [/]&gt;

       Do you need a username and password to access the HTTP server?

       0) &lt;-- Back &lt;--
       1) Yes
       2) No

       &gt; 2

       Use a HTTP proxy?

       0) &lt;-- Back &lt;--
       1) Yes
       2) No

       &gt; 2
       Loading Installation System (1/6) -	100%
       squashfs: version 4.0 (2009/01/31) Phillip Lougher
       Loading Installation System (2/6) -	100%
       Loading Installation System (3/6) -	100%
       Loading Installation System (4/6) -	100%
       Loading Installation System (5/6) -	100%
       Loading Installation System (6/6) -	100%
       Reading Driver Update...

       No new Driver Updates found

       Select the display type.

       0) &lt;-- Back &lt;--
       1) X11
       2) VNC
       3) SSH
       4) ASCII Console

       &gt; 3

       Enter your temporary SSH password. (Enter &#39;+++&#39; to abort).
       &gt; (doesn&#39;t echo, you&#39;ll need this below)

       starting hald... ok
       starting syslogd (logging to /dev/tty4)... ok
       starting klogd... ok
       sshd found, prepare remote login
       generating SSH keys  ...
       ssh-keygen: generating new host keys: RSA1 RSA DSA ECDSA ED25519
       Starting SSH daemon  ...

       eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
	   link/ether 08:00:2f:00:11:22 brd ff:ff:ff:ff:ff:ff
	       inet 10.0.2.15/24 brd 10.0.2.255 scope global eth0
	       inet6 fe80::a00:2fff:fe00:1122/64 scope link
		      valid_lft forever preferred_lft forever

		***  sshd has been started  ***


		***  login using &#39;ssh -X root@10.0.2.15&#39;  ***
		***  run &#39;yast&#39; to start the installation  ***

       At this, point, open a new terminal. SSH to locahost and not the
       reported 10.0.2.15 IP, since QEMU is forwarding the guests port 22 to
       the hosts port 10022. Do not pass the -X flag to ssh, since it will
       try to do X forwarding of the installer, and its painfully slow.

       $ ssh -p 10022 root@localhost
       $ yast

       From here, its a normal SUSE install. It will halt when its finished,
       and you can then start qemu without the supplied kernel and initrd,
       since it will boot from the root disk now:

       $ qemu-system-s390x -M s390-ccw-virtio -m 1024 -smp 1 -nographic \
	 -drive file=SLES-11-SP4-s390x.qcow2,format=qcow2,if=none,id=drive-virtio-disk0 \
	 -device virtio-blk-ccw,drive=drive-virtio-disk0,id=virtio-disk0 \
	 -netdev user,id=mynet0,hostfwd=tcp::10022-:22 \
	 -device virtio-net-ccw,netdev=mynet0,id=net0,mac=08:00:2F:00:11:22,devno=fe.0.0001

       It will ask you to run  /usr/lib/YaST2/startup/YaST2.ssh , and after
       that is finished, you can reboot into a working system. Enjoy!

</content></entry>
</feed>

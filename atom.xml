<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>johnwfinigan.github.io</title>
  <link rel="self" type="application/atom+xml" href="https://johnwfinigan.github.io/atom.xml"/>
  <updated>2025-05-07T00:00:00Z</updated>
  <author>
    <name>John Finigan</name>
  </author>
  <id>tag:johnwfinigan.github.io,2015-09-14:blog</id>
  <rights> Copyright 2015-2023 John Finigan </rights>
<entry>
<id>tag:johnwfinigan.github.io,2025-05-07:Trouble-Enabling-Vertex-AI-With-Terraform</id>
<title>Trouble Enabling Vertex AI With Terraform</title>
<updated>2025-05-07T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Trouble Enabling Vertex AI With Terraform
     Since at least 2023 some of my older Terraform code bases for Google
     Cloud would fail when trying to enable the Vertex AI
     (aiplatform.googleapis.com) or Vertex AI Workbench
     (notebooks.googleapis.com) APIs on a project. The projects were created
     with the Google Project Factory Terraform module, but I suspect that the
     problem is not inherent to that module. Enabling the API by hand would
     succeed, so while this was annoying, that is how I would work around it.
     After a one time manual API enablement, the Terraform code would apply
     cleanly.

     The error code, SERVICECONFIGNOTFOUNDORPERMISSIONDENIED, seemed like it
     would be a good lead, but didn&#39;t lead to many search engine hits. There
     were some suggestions that the identity Terraform was running as needed
     to have Service Usage Admin on the project, and while that may be needed,
     it was not a fix in my case.

     After many failed attempts to identify the cause, the solution was to
     update all of the Google Terraform modules involved in the apply. I don&#39;t
     think any given module&#39;s implementation was the problem. Each module sets
     constraints on what versions of the Google terraform provider it will
     accept. Terraform calculates a lowest common denominator and does the
     apply using that version of the provider. Here, one of the modules was
     apparently constraining it to use a version of the provider that was too
     old to handle these APIs. I don&#39;t use many 3rd party modules, and this
     part of the code was in need of maintenance, so I ended up updating all
     of them to the latest after testing. This was part of an effort to pin
     module versions by Git hash, as an unrelated security improvement, and so
     it was a good time to do it.

     In short, find the modules you have in your Terraform code which use the
     Google provider, and update them if they are old. Provider version
     constraints in the old modules may be causing your API enablement to
     fail.

     Example error:

     [
       {
	 &quot;@type&quot;: &quot;type.googleapis.com/google.rpc.PreconditionFailure&quot;,
	 &quot;violations&quot;: [
	   {
	     &quot;subject&quot;: &quot;?error_code=220002\u0026services=aiplatform.googeapis.com&quot;,
	     &quot;type&quot;: &quot;googleapis.com&quot;
	   }
	 ]
       },
       {
	 &quot;@type&quot;: &quot;type.googleapis.com/google.rpc.ErrorInfo&quot;,
	 &quot;domain&quot;: &quot;serviceusage.googleapis.com&quot;,
	 &quot;metadata&quot;: {
	   &quot;services&quot;: &quot;aiplatform.googeapis.com&quot;
	 },
	 &quot;reason&quot;: &quot;SERVICE_CONFIG_NOT_FOUND_OR_PERMISSION_DENIED&quot;
       }
     ]
     , forbidden

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2024-08-10:Azure-CLI-setup-of-SPN-to-Create-Subscriptions</id>
<title>Azure CLI setup of SPN to Create Subscriptions</title>
<updated>2024-08-10T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Azure CLI setup of SPN to Create Subscriptions
     Microsoft documents &lt;https://learn.microsoft.com/en-
     us/azure/developer/terraform/authenticate-to-azure-with-service-
     principle&gt; how to authenticate Terraform to Azure using a SPN (Service
     Principal Name) identity. To do anything useful with that SPN, you will
     need to give it rights on the subscriptions it will be used to manage.

     You may want Terraform to be able to create new subscriptions when
     running as your SPN. If you are using Enterprise Agreement billing, this
     is a little tricky since SPNs cannot create subscriptions under an EA
     account unless the SPN has been granted a Subscription Creator role which
     can only be granted via an Azure REST call.

     Below is a script for doing that using CLI only. It assumes you already
     have a SPN and know its Client ID, as well as identifier numbers about
     your EA billing account that you can find in the Azure web UI. None of
     these values change, so as part of setup, you can find them and record
     them for use in scripts.

     I developed this based on Microsoft docs here
     &lt;https://learn.microsoft.com/en-us/azure/cost-management-
     billing/manage/assign-roles-azure-service-principals#assign-the-
     subscription-creator-role-to-the-spn&gt; and here
     &lt;https://learn.microsoft.com/en-us/rest/api/billing/role-
     assignments/put?view=rest-billing-2019-10-01-preview&amp;tabs=HTTP&gt; and a
     Stack Overflow answer here
     &lt;https://stackoverflow.com/questions/56645576/activation-of-service-
     principal-as-account-owner-in-the-ea-portal-via-powershell&gt;.

     Script below is for Enterprise Agreement billing only. If you are using
     &quot;Credit Card&quot; type billing it won&#39;t work for you.

     #!/bin/bash

     set -euxo pipefail
     which uuidgen # will exit due to set -e if no uuidgen, which is part of util-linux
     which az

     # Terraform reads these variable names to auth as SPN
     # Client ID is printed out when you create your SPN
     # Tenant ID is global to your tenant
     # You must set these for this script to work
     ARM_TENANT_ID=a-uuid-goes-here
     ARM_CLIENT_ID=a-uuid-goes-here

     # Info about your Enterprise Agreement billing/enrollment
     # You must set these for this script to work
     billingAccountName=an-integer-goes-here
     enrollmentAccountName=an-integer-goes-here

     # Role definition id is documented by Microsoft
     # Role assignment name is randomly generated
     roleDefinitionId=a0bcee42-bf30-4d1b-926a-48d21664ef71 #subscriptionCreator
     billingRoleAssignmentName=$(uuidgen)

     # Get the SPN&#39;s Object ID using the SPN&#39;s Client ID
     SPN_ObjectID=$(az ad sp show --id ${ARM_CLIENT_ID?} --query id --output tsv)


     t=$(mktemp)
     cat &lt;&lt;HERE &gt;$t
     {
       &quot;properties&quot;: {
	  &quot;principalID&quot;: &quot;${SPN_ObjectID}&quot;,
	  &quot;principalTenantId&quot;: &quot;${ARM_TENANT_ID}&quot;,
	  &quot;roleDefinitionId&quot;: &quot;/providers/Microsoft.Billing/billingAccounts/${billingAccountName}/enrollmentAccounts/${enrollmentAccountName}/billingRoleDefinitions/${roleDefinitionId}&quot;
       }
     }
     HERE

     az rest --method put --url &quot;https://management.azure.com/providers/Microsoft.Billing/billingAccounts/${billingAccountName}/enrollmentAccounts/${enrollmentAccountName}/billingRoleAssignments/${billingRoleAssignmentName}?api-version=2019-10-01-preview&quot; --body @${t}

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2023-10-13:Quick-XFS-Quota-Notes</id>
<title>Quick XFS Quota Notes</title>
<updated>2023-10-13T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Quick XFS Quota Notes
     On multiuser systems, filesystem quotas are a low maintenance way to stop
     a single user from filling a partition. XFS quota support is good, and
     XFS project quotas provide a way to limit the growth of a directory tree
     even if it is not on a separate filesystem. These notes are really just a
     quick summary of man xfs_quota. Tested on RHEL 7 and up.

   Per user quotas
     xfs_quota -x -c &#39;limit bsoft=2g bhard=2g -d&#39; /tmp		# set a default space quota, applies to each user but not root

     xfs_quota -x -c &#39;limit bsoft=10g bhard=10g myuser&#39; /tmp	# override the default for a particular user

     xfs_quota -x -c &#39;report -h&#39; /tmp				# display report, find users who have hit quota

     For user quotas to work, the filesystem must be mounted with the uquota
     option.

     /dev/mapper/vg00-tmp   /tmp		   xfs	   defaults,uquota	  0 0

   Project Quotas
     This lets you set a quota on a directory tree and tracks usage
     independent of what user owns the data.

     In this example, the directory tree being limited is /home, and /home is
     part of the / filesystem. 10 is a project number that is arbitrary but
     needs to be unique per project quota.

     xfs_quota -x -c &#39;project -s -p /home 10&#39; /		       # define a project number for /home

     xfs_quota -x -c &#39;limit -p bhard=2g 10&#39; /		       # set an overall limit on that project

     For project quotas to work, the filesystem must be mounted with the
     pquota option. You can enable multiple quota types on the same
     filesystem.

     /dev/mapper/vg00-home   /home		     xfs     defaults,pquota	    0 0

   Special steps if enabling quotas on root
     Tested only on RHEL 7.

     If you are enabling project quotas on the root filesystem, add
     rootflags=pquota to /etc/default/grub (append to GRUB_CMDLINE_LINUX)

     For example, editing non-interactively:

     sed -e &#39;s/^GRUB_CMDLINE_LINUX=&quot;\(.\+\)&quot;$/GRUB_CMDLINE_LINUX=&quot;\1 rootflags=pquota&quot;/&#39; /etc/default/grub

     and then run grub2-mkconfig -o [...grub.cfg] to update kernel boot
     command options.

     For example,

     grub2-mkconfig -o	/boot/efi/EFI/centos/grub.cfg

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2023-10-13:Limiting-CPU-and-Memory-on-User-Login-Sessions-with-Systemd</id>
<title>Limiting CPU and Memory on User Login Sessions with Systemd</title>
<updated>2023-10-13T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Limiting CPU and Memory on User Login Sessions with Systemd
     On multiuser Linux systems you may need to limit the memory and CPU use
     of interactive users so that one user cannot hog or crash the box.
     There&#39;s several ways to do this, but I find the systemd approach to be
     the cleanest. On older systems like RHEL 7 you can still use cgroups with
     the cgred approach, and on really old systems there is still ulimit

     This approach uses a template for the systemd slice that a user&#39;s login
     session is added to on login. This allows you to use standard systemd
     resource controls as defined in the systemd documentation
     &lt;https://www.freedesktop.org/software/systemd/man/systemd.resource-
     control.html&gt;

     At /etc/systemd/system/user-.slice.d/50-userlimits.conf create the
     following:

     [Slice]
     MemoryMax=50G
     TasksMax=512
     CPUQuota=400%

     For a given user UID, you can then check some stats, including memory
     usage,  on their slice using systemctl - for example:

     systemctl status user-12345.slice

     Tested on RHEL 8 and 9.

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2023-06-30:Using-IAP-TCP-Forwarding-to-Build-Compute-Images-with-Packer-in-Google-Cloud-Build</id>
<title>Using SSH over IAP TCP Forwarding to Build Compute Images with Packer in Google Cloud Build</title>
<updated>2023-06-30T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Using SSH over IAP TCP Forwarding to Build Compute Images with Packer in
     Google Cloud Build
     Previously I wrote &lt;./Using-IAP-SSH-to-Run-Ansible-in-Google-Cloud-
     Build.html&gt; about using Google Identity Aware Proxy based SSH to run
     Ansible within Google Cloud Build without needing a firewall penetration
     or network peering between Cloud Build and the target VMs, and without
     the target VMs needing public IPs. In this post I&#39;ll show how to use IAP
     TCP forwarding to build compute images using Hashicorp Packer running in
     Cloud Build, also without direct networking between Cloud Build and the
     ephemeral VM that Packer uses for the image build.

     That previous post documents the necessary IAM and firewall rules needed
     for full IAP SSH to work, however here we are using IAP TCP forwarding,
     and SSH authentication is being handled by Packer itself instead of
     OSLogin, so only a subset of the requirements need to be met: the GCE
     firewall must allow IAP to communicate with the VM, and the Cloud Build
     service account must have IAM rights to start a tunnel to the VM
     (roles/iap.tunnelResourceAccessor)

     Example cloudbuild.yaml - Click here &lt;./assets/cloudbuild-packer.yml&gt; to
     view raw.

     steps:
       - name: &#39;hashicorp/packer&#39;
	 entrypoint: sh
	 args:
	   - &#39;-c&#39;
	   - |
	       cp $(which packer) /workspace/
	       chmod 555 /workspace/packer

       - name: &#39;gcr.io/google.com/cloudsdktool/google-cloud-cli:slim&#39;
	 env:
	   - &#39;PACKER_NO_COLOR=true&#39;
	 entrypoint: bash
	 args:
	   - &#39;-c&#39;
	   - |
	       set -euo pipefail
	       $(gcloud info --format=&quot;value(basic.python_location)&quot;) -m pip install numpy
	       python3 -m pip install ansible
	       touch ./log
	       ( while ! grep -Fq &quot;Instance has been created&quot; ./log ; do
		   echo &quot;waiting to start tunnel&quot; ;
		   sleep 5 ;
		 done ;
		 sleep 60 ;
		 gcloud compute start-iap-tunnel packer-${BUILD_ID} 22 --local-host-port=127.0.0.1:22222 --zone=${_BUILD_ZONE} ) &amp;
	       /workspace/packer build \
		 -var zone=${_BUILD_ZONE} \
		 -var instance_name=packer-${BUILD_ID} \
		 my_packerfile.pkr.hcl |&amp; tee ./log

     options:
       logging: CLOUD_LOGGING_ONLY
     timeout: 3600s

     Essentially, IAP TCP tunnelling is used to make port 22 on the target VM
     appear at port 22222 inside the Cloud Build runtime, and directives are
     added to the packerfile to link this all together, as shown below. In
     Cloud Build, $BUILD_ID is a built-in variable, but $_BUILD_ZONE is a
     user-supplied substitution that I am showing here since IAP tunneling and
     the compute instance have to be coordinated regarding the zone and the
     build VM&#39;s name. Your packerfile will contain something like this:

     source &quot;googlecompute&quot; &quot;my_build&quot; {
       ...
       ...
       zone		       = &quot;${var.zone}&quot;
       disable_default_service_account = true
       instance_name	       = &quot;${var.instance_name}&quot;
       ssh_host		       = &quot;127.0.0.1&quot;
       ssh_port		       = &quot;22222&quot;
       pause_before_connecting = &quot;60s&quot;
       metadata = {
	 enable-oslogin = &quot;FALSE&quot;
       }
       ...
       ...
     }

     Notably, this is not the prettiest shell scripting. There are probably
     race conditions in it, and some of the inserted waits may not actually be
     needed to avoid them. However, I&#39;ve run a few dozen Linux image builds
     successfully using this code, and have not experienced a failure to
     connect yet.

     Unlike my Ansible example, here I chose to rely on no custom containers
     and assemble everything needed using well known images.

     As a bonus, here is some terraform you may be able to adapt to set up
     your firewall and IAM to allow IAP tunnelling to your VMs:

     resource &quot;google_compute_firewall&quot; &quot;allow-iap-ssh&quot; {
       name    = &quot;allow-iap-ssh&quot;
       network = google_compute_network.FIXME.name
       allow {
	 protocol = &quot;tcp&quot;
	 ports	  = [&quot;22&quot;]
       }
       source_ranges = [&quot;35.235.240.0/20&quot;]
       priority	     = &quot;1000&quot;
     }

     module &quot;image-cloudbuild&quot; {
       source	    = &quot;terraform-google-modules/service-accounts/google&quot;
       names	    = [&quot;image-cloudbuild&quot;]
       display_name = &quot;image-cloudbuild&quot;
       project_roles = [
	 &quot;FIXME_PROJECT=&gt;roles/cloudbuild.builds.builder&quot;,
	 &quot;FIXME_PROJECT=&gt;roles/compute.instanceAdmin.v1&quot;,
	 &quot;FIXME_PROJECT=&gt;roles/compute.networkUser&quot;,
	 &quot;FIXME_PROJECT=&gt;roles/iap.tunnelResourceAccessor&quot;,
       ]
     }

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2023-05-18:Polyinstantiating-tmp-directories-in-RHEL-8</id>
<title>Polyinstantiating tmp Directories in RHEL 8</title>
<updated>2023-05-18T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Polyinstantiating tmp Directories in Modern RHEL
     Polyinstantiation is a Linux security feature for giving each user a
     private virtual copy of normally globally visible directories such as
     /tmp. Linux kernel namespaces are used to map each user&#39;s view of the
     polyinstantiated directory to a separate directory in the default
     namespace. This is useful for eliminating the side channel that allows
     users to defeat administratively set file permissions and share data
     inappropriately by copying it into world writable directories such as
     /tmp. When /tmp is polyinstantiated, the user sees his own data in /tmp
     and nothing else.

     This is similar to the systemd PrivateTmp hardening feature, but
     implemented differently.

     I have never had good luck with the instructions generally found online
     for polyinstantiation, such as these
     &lt;https://www.redhat.com/en/blog/polyinstantiating-tmp-and-vartmp-
     directories&gt;. They&#39;ve generally resulted in a broken system for me. The
     following works for me on RHEL 8. I haven&#39;t modified pam.d files because
     namespace support was already there by default. I did not manually create
     the polyinstantiation roots, because I am using the automatic creation
     feature below.

     ---
     - become: yes
       hosts: all
       tasks:

       - name: polyinstantiate temp directories
	 blockinfile:
	   path: /etc/security/namespace.conf
	   block: |
	     /tmp     /tmp/tmp-inst/	     level:create=0000,root,root   root,adm
	     /var/tmp /var/tmp/tmp-inst/     level:create=0000,root,root   root,adm
	     /dev/shm	 /dev/shm/shm-inst/  tmpfs:create=0000,root,root:mntopts=nodev,nosuid,size=128M	  root,adm

       - name: set polyinstantiation selinux boolean
	 seboolean:
	   name: polyinstantiation_enabled
	   state: true
	   persistent: true

     You&#39;ll note that /dev/shm is created differently, as a tmpfs mount.
     Trying to create it via level in the existing /dev/shm produced a broken
     system.

     The namespace.conf mntopts syntax is only supported on tmpfs, but I am
     mounting the underlying global /tmp and /var/tmp with nodev,nosuid also,
     and this carries into the polyinstantiated mounts that are rooted there.

     This config probably depends on SELinux.

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2023-05-08:Using-IAP-SSH-to-Run-Ansible-in-Google-Cloud-Build</id>
<title>Using IAP SSH to run Ansible in Google Cloud Build Triggers</title>
<updated>2023-05-08T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Using IAP SSH to run Ansible in Google Cloud Build Triggers
     Google pitches Cloud Build as the &quot;serverless CI/CD platform&quot; for Google
     Cloud (GCP). It&#39;s easy to use for infrastructure tasks like deploying
     cloud infrastructure using Terraform. This is great for ensuring that
     your Terraform build environment is repeatable and not tied to implicit
     state on a build machine.

     When a Cloud Build run is triggered, GCP runs a container of your
     choosing on a network that is, by default, not part of your customer-
     controlled VPC network. By default, traffic egressing from your Cloud
     Build will be seen by your instances as originating from a public IP of
     Google&#39;s choosing. This can be a problem if you want to use Cloud Build
     to run Ansible to configure your instances, since Ansible will typically
     log in to your instances using SSH. You could allow SSH traffic in from
     any public IP, but this is bad security practice if your only reason for
     doing this is admin access.

     Another option is to do the config necessary to give Cloud Build access
     to your customer-controlled VPC network, by using Private Pools, which
     give a limited amount of control over the machine size and networking of
     the virtual hardware your builds run on. However, this requires some
     configuration work &lt;https://cloud.google.com/build/docs/private-
     pools/set-up-private-pool-to-use-in-vpc-network&gt;, including the use of
     Private Services Access to connect to your VPC network by VPC peering.
     Personally, I don&#39;t find this appealing. It injects a fair amount of
     accidental complexity for this simple use case.

     But, there is another way. Google Cloud offers something called Identity
     Aware Proxy (IAP), which allows you to tunnel TCP traffic from anywhere
     to inside your VPC network, without your instances having public IPs. The
     gcloud compute ssh command has built in IAP support. I use this
     extensively for general admin access. It generally &quot;just works&quot;.

     All that remains is to plumb IAP SSH into Ansible, and do it in such a
     way that it can run in Cloud Build.

     The strategy here is to get Ansible to use gcloud compute ssh as its SSH
     provider, and to pass inventory between them in such a way that gcloud
     can recognize the instances, which essentially means using the instance
     name as defined in GCE.

     Below is the Dockerfile for a runtime container that is compatible with
     Cloud Build:

     FROM gcr.io/google.com/cloudsdktool/google-cloud-cli:slim

     RUN $(gcloud info --format=&quot;value(basic.python_location)&quot;) -m pip install numpy
     RUN python3 -m pip install ansible

     ENV ANSIBLE_SSH_EXECUTABLE=/bin/g-ssh.sh
     ENV ANSIBLE_CONFIG=/etc/g-ansible.cfg

     COPY g-ssh.sh $ANSIBLE_SSH_EXECUTABLE
     COPY g-ansible.cfg $ANSIBLE_CONFIG

     This is a Debian-based Google Cloud SDK container. In my testing, an
     Alpine container had significantly worse performance for Ansible over IAP
     SSH. Numpy is installed to speed up IAP forwarding performance. In my
     testing, this improved performance significantly.

     Inside this container is g-ssh.sh, where the IAP SSH magic happens.

     #!/bin/bash

     umask 0077

     # generate an ephemeral ssh key. exists only for this build step
     if [ ! -f &quot;$HOME/.ssh/google_compute_engine&quot; ] ; then
       mkdir -p &quot;$HOME/.ssh&quot;
       ssh-keygen -t rsa -b 3072 -N &quot;&quot; -C &quot;cloudbuild-$(date +%Y-%m-%d-%H-%M-%S)&quot; -f $HOME/.ssh/google_compute_engine
     fi

     #
     # adapted from https://unix.stackexchange.com/questions/545034/with-ansible-is-it-possible-to-connect-connect-to-hosts-that-are-behind-cloud-i
     #

     # get the two rightmost args to the script
     host=&quot;${@: -2: 1}&quot;
     cmd=&quot;${@: -1: 1}&quot;

     # controlmasters is a performance optimization, added because gcloud ssh initiation is relatively slow
     mkdir -p /workspace/.controlmasters/
     # stagger parallel invocations of gcloud to prevent races - not clear how useful
     flock /workspace/.lock1 sleep 1

     # note that the --ssh-key-expire-after argument to gcloud is set, meaning that the ephemeral key
     # will become invalid in OSLogin after one hour. Otherwise will end up with dozens of junk keys in OSLogin
     gcloud_args=&quot; --ssh-key-expire-after=1h --tunnel-through-iap --quiet --no-user-output-enabled -- -C -o ControlPath=/workspace/.controlmasters/%C -o ControlMaster=auto -o ControlPersist=300 -o PreferredAuthentications=publickey -o KbdInteractiveAuthentication=no -o PasswordAuthentication=no -o ConnectTimeout=20 &quot;

     # project and zone must be already set using gcloud config
     exec gcloud compute ssh &quot;$host&quot; $gcloud_args &quot;$cmd&quot;

     Note that an ephemeral SSH key is generated but has an expiration set.
     This key is lost when the build ends, and GCP will remove it from the
     authorized keys list when it expires after one hour. This is done using
     the --ssh-key-expire-after argument to gcloud. Thus there are no long
     lived SSH credentials involved here. I believe this SSH key management
     feature depends on the use of Google OSLogin on the instances.

     This leads the broader subject of roles and identity. The identity that
     is logging in to the instance and running Ansible commands will be the
     service account that your Cloud Build build runs as. This setup depends
     on that account being a valid OSLogin + IAP instance accessor account.
     That requires:

     o	There must be a firewall rule on your VPC networks that allows IAP SSH
	&lt;https://cloud.google.com/iap/docs/using-tcp-forwarding#create-
	firewall-rule&gt; to reach instance private IPs from Google&#39;s designated
	IAP origination range. The instances do not require public IPs.

     o	The instances must have OSLogin Enabled
	&lt;https://cloud.google.com/compute/docs/oslogin/set-up-oslogin&gt;

     o	The Cloud Build service account you use must have the following roles:

	o  roles/compute.osAdminLogin on the instance or project
	o  roles/iam.serviceAccountUser on the instance&#39;s service account
	o  roles/iap.tunnelResourceAccessor on the project or tunnel

     I have not tested with the default Cloud Build service account, but only
     with cloud build running as a custom service account. The Service Account
     User role may seem surprising, but OSLogin requires this for any account
     to be able to log in. I believe it is because any account that logs in to
     an instance can make requests using its service account, so this
     formalizes the relationship.

     Finally, in your container, you need a small Ansible config file, g-
     ansible.cfg. I&#39;ve been using this one for Ansible+IAP for years and don&#39;t
     remember all the details, but some tweaks were required to get file
     transfer (eg. Ansible copy:) to work reliably. You may have to change the
     default interpreter_python based on the Linux version you run in your
     instances. I had to limit forks for stability, but have not extensively
     debugged to see what could be done to speed this up again.

     [ssh_connection]
     pipelining = True
     ssh_args =
     transfer_method = piped

     [defaults]
     forks = 1
     interpreter_python = /usr/bin/python3

     Here&#39;s a sample cloudbuild.yaml that uses this container to run Ansible
     through IAP in Cloud Build:

     steps:

     - id: &#39;test ansible&#39;
       name: &#39;us-central1-docker.pkg.dev/my_project/cloudbuild-containers/cloudbuild-ansible-iap&#39;
       entrypoint: &#39;/bin/bash&#39;
       args:
       - &#39;-c&#39;
       - |
	 gcloud config set project my_example_project;
	 gcloud config set compute/zone us-central1-a ;
	 ansible-playbook -i ./test.ini test.yml

     Note that you must set the project and zone in your build step. That is
     how g-ssh.sh knows how to find your instances by name.

     This has worked well for me. Speed has been acceptable, but a more
     optimized build might create the IAP tunnels directly instead of using
     the gcloud compute ssh helper. This would allow using unwrapped openssh
     to access the instances, but would require some more complex state
     tracking that I have not yet spent time on.

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2023-02-05:Creating-a-Minimal-deb-Repo-From-Scratch</id>
<title>Creating a Minimal apt Repo From Scratch</title>
<updated>2023-02-05T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Creating a Minimal apt Repo From Scratch
     Let&#39;s say you want to create an apt repo for debs you have built
     yourself. I spent an hour or two cobbling together these minimalist
     instructions from various internet sources. The sources I found seemed to
     be either outdated enough that they were no longer correct for recent
     tooling and distros, or more complex than what I was trying to accomplish
     warranted. This is tested against Ubuntu 22.04 clients and servers.
     Notably, this method has no directory structure inside the repo at all.
     All files are under the top level directory of the repo.

   Create a gpg key for signing
     Use the gpg shipped with Ubuntu 22.04 or whatever client distro you plan
     to support. Set the Real Name field of your key to whatever you like.
     Here, it&#39;s called Mirror Signing Key. Once generated, export your key
     using the fingerprint shown when it was generated.

     gpg --armor --export 73341B91FEC7DCBC8316EE01E45BEE2E63B81095 &gt; MirrorSigningKey.pub

     This exported key will be copied to your apt clients for apt metadata
     verification.

     Example gpg session at the bottom of this post. In short, it&#39;s OK to
     choose RSA and RSA, and set RSA key size to 4096.

   Populate your repo
     Put your debs in a directory that you intend to serve to your clients.
     Here, it&#39;s called $repo_dir

   Install repo build tools
     Ensure you have the packages dpkg-deb and apt-utils installed, for the
     next step.

   Create your apt repo
     #!/bin/bash

     set -eu

     repo_dir=/path/to/your/repo/directory
     cd &quot;$repo_dir&quot;

     rm -vf Release Release.gpg InRelease
     dpkg-scanpackages --arch amd64 . &gt; Packages
     apt-ftparchive release . &gt; Release

     echo Enter Passphrase
     read pass
     gpg  --pinentry-mode loopback --digest-algo SHA512 --batch --yes --no-tty --passphrase $pass --default-key &#39;Mirror Signing Key&#39; -abs &lt; Release &gt; Release.gpg
     gpg  --pinentry-mode loopback --digest-algo SHA512 --batch --yes --no-tty --passphrase $pass --default-key &#39;Mirror Signing Key&#39; -abs --clearsign &lt; Release &gt; InRelease
     unset pass

     The awkward method of getting the passphrase to gpg is not appropriate
     for use on untrusted systems, since the passphrase can be read out of
     /proc while gpg is executing. Proper gpg pin entry methods seem
     especially fragile on headless systems, and after an hour of frustration
     trying to debug pinentry, I resorted to it. Someone with better gpg
     skills could do better. On the other hand, it&#39;s easy to see how this
     could be modified to work with a secret manager in a CI system - just
     replace the read call with a call to your secret manager.

   Set up your clients
     On your clients, create the repo definition file and the public signing
     key file in /etc/apt

     install -o root -g root -m0644 &lt;(echo &#39;deb [signed-by=/etc/apt/trusted.gpg.d/MirrorSigningKey.pub] https://example.org/your/repo ./&#39;) /etc/apt/sources.list.d/your_repo.list

     install -o root -g root -m0644 MirrorSigningKey.pub /etc/apt/trusted.gpg.d/

     Note that I&#39;m using install here as an all-in-one way to copy data while
     ensuring that permissions are reasonable. Your preferred way to do that
     should be fine, regardless.

   Done
     When you add, update, or remove packages from your repo, simply rerun the
     repo generation script above.

   Appendix: GPG sample session
     john@s:~$ gpg --full-generate-key
     gpg (GnuPG) 2.2.27; Copyright (C) 2021 Free Software Foundation, Inc.
     This is free software: you are free to change and redistribute it.
     There is NO WARRANTY, to the extent permitted by law.

     gpg: directory &#39;/home/john/.gnupg&#39; created
     gpg: keybox &#39;/home/john/.gnupg/pubring.kbx&#39; created
     Please select what kind of key you want:
	(1) RSA and RSA (default)
	(2) DSA and Elgamal
	(3) DSA (sign only)
	(4) RSA (sign only)
       (14) Existing key from card
     Your selection? 1
     RSA keys may be between 1024 and 4096 bits long.
     What keysize do you want? (3072) 4096
     Requested keysize is 4096 bits
     Please specify how long the key should be valid.
	      0 = key does not expire
	   &lt;n&gt;	= key expires in n days
	   &lt;n&gt;w = key expires in n weeks
	   &lt;n&gt;m = key expires in n months
	   &lt;n&gt;y = key expires in n years
     Key is valid for? (0)
     Key does not expire at all
     Is this correct? (y/N) y

     GnuPG needs to construct a user ID to identify your key.

     Real name: Mirror Signing Key
     Email address: mirrorsignkey@example.org
     Comment:
     You selected this USER-ID:
	 &quot;Mirror Signing Key &lt;mirrorsignkey@example.org&gt;&quot;

     Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O
     We need to generate a lot of random bytes. It is a good idea to perform
     some other action (type on the keyboard, move the mouse, utilize the
     disks) during the prime generation; this gives the random number
     generator a better chance to gain enough entropy.


     gpg: /home/john/.gnupg/trustdb.gpg: trustdb created
     gpg: key E45BEE2E63B81095 marked as ultimately trusted
     gpg: directory &#39;/home/john/.gnupg/openpgp-revocs.d&#39; created
     gpg: revocation certificate stored as &#39;/home/john/.gnupg/openpgp-revocs.d/73341B91FEC7DCBC8316EE01E45BEE2E63B81095.rev&#39;
     public and secret key created and signed.

     pub   rsa4096 2023-02-06 [SC]
	   73341B91FEC7DCBC8316EE01E45BEE2E63B81095
     uid		      Mirror Signing Key &lt;mirrorsignkey@example.org&gt;
     sub   rsa4096 2023-02-06 [E]

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2022-09-13:Firewalling-Linux-NFS-Servers-and-OpenBSD-NFS-Clients</id>
<title>Firewalling Linux NFS Servers and OpenBSD NFS Clients</title>
<updated>2022-09-13T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Firewalling Linux NFS Servers and OpenBSD NFS Clients
     OpenBSD supports only NFS v3. It does not support NFS v4. While NFS v4
     requires only port 2049/tcp to be open between clients and servers, NFS
     v3 typically requires three or four ports open between a client and a
     server, and most of these are dynamically chosen and have no fixed port
     number. NFS v3 predates the widespread use of firewalls and was not
     designed to be easily firewalled.

     In order to firewall NFS v3, the server must be configured to run all
     needed NFS v3 services on fixed ports. On an OpenBSD client, no special
     configuration is required on the mount. However, I recommend configuring
     the mount to run over tcp, both to make firewalling easier and for
     general robustness. An OpenBSD fstab entry like the following is enough:

     172.16.1.6:/srv/data /data nfs rw,tcp 0 0

     For the Linux server configuration, I will use Ansible and Firewalld, but
     there&#39;s nothing special about these and it should be easy to adapt this
     solution to any firewall or config management method. Ubuntu 22.04 has
     most NFS server config in /etc/nfs.conf, while older distros such as
     Ubuntu 20.04 require editing multiple config files. I have configurations
     for both, below:

   Linux NFS Server Firewalld config
     NFS client is at 172.16.1.77

     ---
     - name: NFS server firewall config
       become: yes
       hosts: all
       tasks:


       - name: firewalld rich rules
	 ansible.posix.firewalld:
	   rich_rule: &quot;{{ item }}&quot;
	   zone: public
	   permanent: yes
	   immediate: yes
	   state: enabled
	 loop:
	   - rule family=&quot;ipv4&quot; source address=&quot;172.16.1.77/32&quot; port port=&quot;111&quot; protocol=&quot;tcp&quot; accept
	   - rule family=&quot;ipv4&quot; source address=&quot;172.16.1.77/32&quot; port port=&quot;111&quot; protocol=&quot;udp&quot; accept
	   - rule family=&quot;ipv4&quot; source address=&quot;172.16.1.77/32&quot; port port=&quot;50001-50003&quot; protocol=&quot;tcp&quot; accept

   Ubuntu 22.04 NFS Server Config
     /etc/nfs.conf is in ini file format. Ansible ini_file makes it easy to
     edit ini programatically, but if you&#39;re editing by hand, you can easily
     pull out the section name, option (key) names, and values below:

       - name: nfs v3 port locking for ubuntu 22.04 server
	 ini_file:
	   path: /etc/nfs.conf
	   backup: yes
	   section: &quot;{{ item.s }}&quot;
	   option: &quot;{{ item.o }}&quot;
	   value:  &quot;{{ item.v }}&quot;
	 loop:
	   - { s: &quot;statd&quot;, o: &quot;port&quot;, v: &quot;50001&quot; }
	   - { s: &quot;statd&quot;, o: &quot;outgoing-port&quot;, v: &quot;50000&quot; }
	   - { s: &quot;mountd&quot;, o: &quot;port&quot;, v: &quot;50003&quot; }
	   - { s: &quot;lockd&quot;, o: &quot;port&quot;, v: &quot;50002&quot; }
	   - { s: &quot;lockd&quot;, o: &quot;udp-port&quot;, v: &quot;50004&quot; }

     All of the various NFS server-related services must be restarted to have
     this take effect. systemctl restart nfs-server was not sufficient for me.
     After much fooling around, I gave up and rebooted. I believe the problem
     may be that some of these services can listen on multiple ports, and a
     restart causes them to start listening on their new ports, but not
     abandon their old ports. Which port a new mount request gets is then
     unpredictable.

     Note that we are pinning the lockd UDP port to 50004 for thoroughness,
     but the tcp-only client will not use it.

   Ubuntu 20.04 NFS Server Config
     This configuration, or a close variation on it, should work for any older
     Ubuntu or RHEL version from the last decade or so. Will not take effect
     until all NFS server-related services are restarted.

	- name: port locked nfs config 1
	  lineinfile:
	    line: &#39;options lockd nlm_udpport=50004 nlm_tcpport=50002&#39;
	    path: /etc/modprobe.d/nfs-lockd.conf
	    mode: 0644
	    owner: root
	    group: root
	    backup: yes
	    create: yes

	- name: port locked nfs config 2
	  lineinfile:
	    line: &#39;STATDOPTS=&quot;--port 50001 --outgoing-port 50000&quot;&#39;
	    path: /etc/default/nfs-common
	    backup: yes
	    regexp: &#39;^STATDOPTS=&#39;


	- name: port locked nfs config 3
	  lineinfile:
	    line: &#39;RPCMOUNTDOPTS=&quot;--manage-gids --port 50003&quot;&#39;
	    path: /etc/default/nfs-kernel-server
	    backup: yes
	    regexp: &#39;^RPCMOUNTDOPTS=&#39;

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2015-09-17:installing-nagios-from-epel-on-centos-7-with-gmail-notifications</id>
<title>Minimum viable install: Nagios from epel on Centos 7, with Gmail notifications</title>
<updated>2015-09-17T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Minimum viable install: Nagios from epel on Centos 7, with Gmail
     notifications
     (Historical post - this is outdated, but parts are likely still useful.
     It begs for scripting and templating)

     This is how I got basic Nagios running on Centos 7, using the Nagios
     3.5.1 packages in EPEL &lt;https://fedoraproject.org/wiki/EPEL&gt;. This is
     basically a reinterpretation of the guide at the Nagios Fedora Quickstart
     &lt;https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/3/en/quickstart-
     fedora.html&gt;, except we are installing from packages and not source, and
     a lot of the paths and commands in Centos 7 are different.

     First, to get a basic install running:

   As root:
      rpm -i https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
      yum install  mod_ssl git nagios*

      chkconfig nagios on
      systemctl enable httpd
      systemctl enable firewalld
      service firewalld start
      firewall-cmd --zone=public --permanent --add-service=http
      firewall-cmd --zone=public --permanent --add-service=https

     You can&#39;t add the permanent firewalld rules unless firewalld is already
     running, so we started it.

   Next, edit /etc/httpd/conf.d/nagios.conf
      Remove 2 instances of:

      AuthType Basic
      AuthUserFile /etc/nagios/passwd

      Replace with:

      AuthType Digest
      AuthUserFile /etc/nagios/digest-passwd


      Optionally, uncomment 2 instances of SSLRequireSSL

     This accomplishes two things. Firstly, it changes the HTTP authentication
     method so passwords are never sent in cleartext, but rather in hashed
     form. Secondly, and optionally, it denies access to Nagios over http, and
     enforces the use of https. Edit /etc/httpd/conf.d/ssl.conf to point it to
     a signed ssl certificate if desired.

   As root:
     In order to use digest passwords instead of plaintext passwords, we have
     to create the digest password file:

      htdigest -c /etc/nagios/digest-passwd &quot;Nagios Access&quot; nagiosadmin
      # will prompt for password

      chown root:apache /etc/nagios/digest-passwd
      chmod 640 /etc/nagios/digest-passwd

     Next, reboot! This is partially out of laziness so we dont have to start
     the services manually, and partially due to a virtuous desire to see if
     our services autostart as requested.

     reboot

     At this point you should be able to log in to Nagios on both http and
     https.

   Next, edit /etc/nagios/objects/contacts.cfg
     Find the line containing CHANGE THIS TO YOUR EMAIL ADDRESS , and do what it asks.

     Append a sample notification contact and contact group to the end of the file:
     Don&#39;t forget to change the email address below.

     define contactgroup {
	     contactgroup_name	     testgroup
	     alias		     Test Contact Group
	     members		     testsysadmin
     }

     define contact {
	     contact_name		     testsysadmin
	     alias			     Test-Sysadmin
	     service_notification_period     24x7
	     host_notification_period	     24x7
	     service_notification_options    c,r
	     host_notification_options	     d,r
	     service_notification_commands   notify-service-by-email
	     host_notification_commands	     notify-host-by-email
	     email			     sysadmin@example.com   &lt;-- CHANGE THIS!
      }

   Define some things to monitor
     By default, what you put in /etc/nagios/conf.d/ gets monitored.

     cd /etc/nagios/conf.d

     This directory starts empty and you can add config files organized
     however you like. Here&#39;s one barebones layout, as an example. Don&#39;t copy
     and paste, since the hostnames are up to you.

     Contents of file /etc/nagios/conf.d/hosts.cfg

     define hostgroup{
	     hostgroup_name  all-servers ; The name of the hostgroup
	     alias	     All Servers ; Long name of the group
     }

     define host{
	     use	     linux-server	     ; Inherit default values from a template
	     host_name	     myserver.example.com    ; The name we&#39;re giving to this host
	     alias	     myserver		     ; A longer name associated with the host
	     address	     10.0.0.10
	     hostgroups	     all-servers	     ; Host groups this host is associated with
     }

     Contents of file /etc/nagios/conf.d/ssh.cfg

     define service{
	 use		 generic-service
	 host_name	 myserver.example.com
	 service_description SSH
	 check_command		 check_ssh
     }

     Contents of file /etc/nagios/conf.d/http.cfg

     define service{
	 use		 generic-service
	 host_name	 myserver.example.com
	 service_description HTTP
	 check_command		 check_http
     }

     define service{
	 use		 generic-service
	 host_name	 myserver.example.com
	 service_description HTTP
	 check_command		 check_http!-S -H mysite.example.com  ; checks https on a different virtual host
     }

     Important fact: these check commands live in /usr/lib64/nagios/plugins,
     and can be run manually, including to get help.

     /usr/lib64/nagios/plugins/check_http -h

   Test your config
     If it works, restart Nagios to pick up config file changes. You will do
     this a lot.

     /usr/sbin/nagios -v /etc/nagios/nagios.cfg
     service nagios restart

   Gmail Notifications
     Finally, let&#39;s get some notifications going. Nagios is ready to notify as
     soon as we have a working mta. I use gmail, and don&#39;t want to run a real
     MTA like sendmail. Taking inspiration from here
     &lt;http://sharadchhetri.com/2013/07/16/how-to-use-email-id-of-gmail-for-
     sending-nagios-email-alerts/&gt;, we will use the ssmtp &quot;send-only sendmail
     emulator&quot;, but moving around binaries like in the linked post makes me
     nervous since the moves will be clobbered by future Centos updates, so
     we&#39;ll use the alternatives system to subsititute ssmtp for sendmail
     instead.

     Be very careful if you are already running a MTA like sendmail on your
     nagios machine and want it to keep working.

     yum install ssmtp
     alternatives --install /sbin/sendmail sendmail /sbin/ssmtp 100

     Then append the following to /etc/ssmtp/ssmtp.conf

     AuthUser=my-nagios-sender@gmail.com   &lt;-- CHANGE THIS!
     AuthPass=password123		   &lt;-- CHANGE THIS!
     FromLineOverride=YES
     mailhub=smtp.gmail.com:587
     UseSTARTTLS=YES

   Done!
     This is a start anyway. You can spend as much time as you like refining
     your config; if anything, the difficulty in getting started is that there
     are so many options.

</content></entry>
<entry>
<id>tag:johnwfinigan.github.io,2015-09-14:installing-suse-linux-sles-11-on-qemus-s390x-emulated-mainframe</id>
<title>Installing SUSE Linux SLES 11 on qemu's s390x emulated mainframe with virtio</title>
<updated>2015-09-14T00:00:00Z</updated>
<author> <name>John Finigan</name> </author>
<content>

   Installing SUSE Linux SLES 11 on qemu&#39;s s390x emulated mainframe with
     virtio
     (Historical post - this is outdated, but parts are likely still useful)

     Installing Linux on an emulated mainframe can be confusing unless you are
     already a mainframer and you have good install docs for your specific
     distro. The traditional choice of emulators is Hercules
     &lt;http://www.hercules-390.eu&gt;, but Hercules is written by mainframers, for
     mainframers, and there is a lot to learn if you want to build your config
     from scratch.

     Traditional mainframe Linux does not install or boot like amd64 Linux.
     Hercules is designed to run traditional mainframe operating systems like
     z/OS, and so presents hardware to the guest that looks like what z/OS
     expects to see. There is a lot to understand if you just want to try your
     program on s390x Linux.

     However, QEMU&#39;s &lt;http://wiki.qemu.org/Main_Page&gt; s390x abilities have
     improved recently due to IBM&#39;s adoption of KVM virtualization on z
     Systems as an alternative to z/VM. Most importantly, recent QEMU has new
     virtio paravirtual IO devices for s390x Linux, meaning that you do not
     need to configure emulated mainframe channel controllers and DASD.

     All of this would not help if mainframe QEMU was only useful for KVM. But
     it&#39;s not: the qemu-system-s390x emulator works just fine.

     I used QEMU 2.4, built from source. Everything else came with my Ubuntu
     15.04 install. I doubt I would have figured any of this out without
     looking at this SHARE presentation by Mark Post
     &lt;https://share.confex.com/share/125/webprogram/Handout/Session17489/know.your.competition.pdf&gt;

     In order for this to work, you will need a mainframe Linux distro that
     supports virtio mainframe disk and network. Since this is a recent
     addition, most mainframe Linux distros do not have a kernel that supports
     it. SUSE Linux Enterprise Server 11 SP4 is new enough. You can get a
     trial here &lt;https://www.suse.com/products/server/download/&gt;

     Too new may also not work: Apparently RHEL 7 and SLES 12 won&#39;t
     &lt;https://lists.gnu.org/archive/html/qemu-devel/2015-08/msg03884.html&gt;.
     The whole linked discussion is worth reading.

     Finally, it&#39;s convenient to do the install over HTTP since real
     mainframes rarely install from CD, so CD is not the path of least
     resistance.

     To prep, get yourself a copy of the SUSE ISO and create a virtual disk
     file for your root drive:

     $ mkdir $HOME/qemu.test ; cd $HOME/qemu.test
     $ qemu-img create -f qcow2 SLES-11-SP4-s390x.qcow2 20G
     $ ls
     SLES-11-SP4-DVD-s390x-GM-DVD1.iso	SLES-11-SP4-s390x.qcow2

     Now, you will need an initrd and kernel to start QEMU with. The initrd is
     on the ISO, but we have to work a little for the kernel:

     $ mkdir mnt
     $ sudo mount ./SLES-11-SP4-DVD-s390x-GM-DVD1.iso ./mnt
     $ cp mnt/boot/s390x/initrd .
     $ mkdir junk ; cd junk
     $ rpm2cpio ../mnt/suse/s390x/kernel-default-base-3.0.101-63.1.s390x.rpm | cpio -ivd
     $ zcat boot/vmlinux-3.0.101-63-default.gz &gt; ../kernel
     $ cd .. ; rm -rf junk

     Finally, serve the install disk&#39;s contents locally using HTTP:

     $ cd ./mnt ; python -m SimpleHTTPServer

     Now, in a new terminal, the moment we&#39;ve all been waiting for::

     $ cd $HOME/qemu.test
     $ qemu-system-s390x -M s390-ccw-virtio -m 1024 -smp 1 -nographic \
       -drive file=SLES-11-SP4-s390x.qcow2,format=qcow2,if=none,id=drive-virtio-disk0 \
       -device virtio-blk-ccw,drive=drive-virtio-disk0,id=virtio-disk0 \
       -netdev user,id=mynet0,hostfwd=tcp::10022-:22 \
       -device virtio-net-ccw,netdev=mynet0,id=net0,mac=08:00:2F:00:11:22,devno=fe.0.0001 \
       -kernel ./kernel -initrd ./initrd

     A couple of networking notes: We are using QEMU&#39;s &quot;user&quot; networking
     option here, which uses QEMU&#39;s internal NAT gateway and DHCP server, but
     is slow. It is zero setup though, which is why we&#39;re using it. The
     hostfwd=tcp::10022-:22 argument forwards port 22 (SSH) on the guest to
     port 10022 on the host. The MAC address is in PR1ME&#39;s space, so make sure
     it does not conflict with any PRIMOS
     &lt;https://en.wikipedia.org/wiki/PRIMOS&gt; systems you may be running.

     Here&#39;s a log of me running through the beginning of the setup once the
     guest is booted. 10.0.2.2 is QEMU&#39;s emulated router and maps to the host.

     &gt;&gt;&gt; Linuxrc v3.3.108 (Kernel 3.0.101-63-default) &lt;&lt;&lt;

     Main Menu

     0) &lt;-- Back &lt;--
     1) Start Installation
     2) Settings
     3) Expert
     4) Exit or Reboot

     &gt; 1

     Start Installation

     0) &lt;-- Back &lt;--
     1) Start Installation or Update
     2) Boot Installed System
     3) Start Rescue System

     &gt; 1

     Choose the source medium.

     0) &lt;-- Back &lt;--
     1) DVD / CD-ROM
     2) Network

     &gt; 2

     Choose the network protocol.

     0) &lt;-- Back &lt;--
     1) FTP
     2) HTTP
     3) HTTPS
     4) NFS
     5) SMB / CIFS (Windows Share)
     6) TFTP

     &gt; 2
     Detecting and loading network drivers

     Automatic configuration via DHCP?

     0) &lt;-- Back &lt;--
     1) Yes
     2) No

     &gt; 1
     Sending DHCP request...
     8021q: adding VLAN 0 to HW filter on device eth0

     Enter the IP address of the HTTP server. (Enter &#39;+++&#39; to abort).
     &gt; 10.0.2.2:8000

     Enter the directory on the server. (Enter &#39;+++&#39; to abort).
     [/]&gt;

     Do you need a username and password to access the HTTP server?

     0) &lt;-- Back &lt;--
     1) Yes
     2) No

     &gt; 2

     Use a HTTP proxy?

     0) &lt;-- Back &lt;--
     1) Yes
     2) No

     &gt; 2
     Loading Installation System (1/6) -      100%
     squashfs: version 4.0 (2009/01/31) Phillip Lougher
     Loading Installation System (2/6) -      100%
     Loading Installation System (3/6) -      100%
     Loading Installation System (4/6) -      100%
     Loading Installation System (5/6) -      100%
     Loading Installation System (6/6) -      100%
     Reading Driver Update...

     No new Driver Updates found

     Select the display type.

     0) &lt;-- Back &lt;--
     1) X11
     2) VNC
     3) SSH
     4) ASCII Console

     &gt; 3

     Enter your temporary SSH password. (Enter &#39;+++&#39; to abort).
     &gt; (doesn&#39;t echo, you&#39;ll need this below)

     starting hald... ok
     starting syslogd (logging to /dev/tty4)... ok
     starting klogd... ok
     sshd found, prepare remote login
     generating SSH keys  ...
     ssh-keygen: generating new host keys: RSA1 RSA DSA ECDSA ED25519
     Starting SSH daemon  ...

     eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
	 link/ether 08:00:2f:00:11:22 brd ff:ff:ff:ff:ff:ff
	     inet 10.0.2.15/24 brd 10.0.2.255 scope global eth0
	     inet6 fe80::a00:2fff:fe00:1122/64 scope link
		    valid_lft forever preferred_lft forever

	      ***  sshd has been started  ***


	      ***  login using &#39;ssh -X root@10.0.2.15&#39;	***
	      ***  run &#39;yast&#39; to start the installation	 ***

     At this, point, open a new terminal. SSH to locahost and not the reported
     10.0.2.15 IP, since QEMU is forwarding the guest&#39;s port 22 to the host&#39;s
     port 10022. Do not pass the -X flag to ssh, since it will try to do X
     forwarding of the installer, and it&#39;s painfully slow.

     $ ssh -p 10022 root@localhost
     $ yast

     From here, it&#39;s a normal SUSE install. It will halt when it&#39;s finished,
     and you can then start qemu without the supplied kernel and initrd, since
     it will boot from the root disk now:

     $ qemu-system-s390x -M s390-ccw-virtio -m 1024 -smp 1 -nographic \
       -drive file=SLES-11-SP4-s390x.qcow2,format=qcow2,if=none,id=drive-virtio-disk0 \
       -device virtio-blk-ccw,drive=drive-virtio-disk0,id=virtio-disk0 \
       -netdev user,id=mynet0,hostfwd=tcp::10022-:22 \
       -device virtio-net-ccw,netdev=mynet0,id=net0,mac=08:00:2F:00:11:22,devno=fe.0.0001

     It will ask you to run  /usr/lib/YaST2/startup/YaST2.ssh , and after that
     is finished, you can reboot into a working system. Enjoy!

</content></entry>
</feed>
